set.seed(831)
# Load libraries
library(topicmodels)
library(lda)
library(slam)
library(stm)
library(ggplot2)
library(dplyr)
library(tidytext)
library(future) # Speed up processing
plan(multicore)
library(tm) # Text mining framework
library(tidyverse) # Data manipulation
library(wordcloud) # For word clouds
library(NLP)
library(ldatuning)
library(readr)
library(igraph)
# Clear up data in global environment
rm(list=ls())
# Load data from csv file
fds_copy_pro <- read_csv("C:/Users/sridh/fds copy pro2.csv")
# Check for NAs
sapply(fds_copy_pro, function(x) sum(is.na(x)))
# Overview of original dataset
str(fds_copy_pro)
sapply(fds_copy_pro, typeof)
# randomly sample 1000 rows & remove unnecessary columns
set.seed(830)
fds_copy_pro_sample <-fds_copy_pro[sample(nrow(fds_copy_pro), 1000), -c(1)]
# Format and transform columns
fds_copy_pro_sample$Platform <- as.factor(fds_copy_pro_sample$Platform)
# Double-check format
sapply(fds_copy_pro_sample, typeof)
# * default parameters
processed <- textProcessor(fds_copy_pro_sample$Comments, metadata = fds_copy_pro_sample,
lowercase = TRUE, #*
removestopwords = TRUE, #*
removenumbers = TRUE, #*
removepunctuation = TRUE, #*
stem = TRUE, #*
wordLengths = c(3,Inf), #*
sparselevel = 1, #*
language = "en", #*
verbose = TRUE, #*
onlycharacter = TRUE, # not def
striphtml = FALSE, #*
customstopwords = NULL, #*
v1 = FALSE) #*
# filter out terms that don’t appear in more than 10 documents,
out <- prepDocuments(processed$documents, processed$vocab, processed$meta, lower.thresh=10)
docs <- out$documents
vocab <- out$vocab
meta <-out$meta
# Check levels
levels(meta$Platform)
set.seed(831)
system.time({
First_STM <- stm(docs, vocab, 15,
prevalence =~ Platform,
data = meta,
seed = 15, max.em.its = 5
)
})
# Plot first Topic Model
plot(First_STM)
set.seed(832)
system.time({
Second_STM <- stm(documents = out$documents, vocab = out$vocab,
K = 18, prevalence =~ Platform ,
max.em.its = 75, data = out$meta,
init.type = "Spectral", verbose = FALSE
)
})
# Plot second Topic Model
plot(Second_STM)
# Find k: Approach 1
set.seed(833)
system.time({
findingk <- searchK(out$documents, out$vocab, K = c(10:30),
prevalence =~ Platform, data = meta, verbose=FALSE
)
})
source("~/.active-rstudio-document")
# Top Words
labelTopics(Third_STM)
# Run final topic model at 20 topics and see how long it takes
set.seed(836)
system.time({
Third_STM <- stm(documents = out$documents, vocab = out$vocab,
K = 20, prevalence =~ Platform,
max.em.its = 75, data = out$meta,
init.type = "Spectral", verbose = FALSE
)
})
set.seed(837)
# Load libraries
library(topicmodels)
library(lda)
library(slam)
library(stm)
library(ggplot2)
library(dplyr)
library(tidytext)
library(future) # Speed up processing
plan(multicore)
library(tm) # Text mining framework
library(tidyverse) # Data manipulation
library(wordcloud) # For word clouds
library(NLP)
library(ldatuning)
library(readr)
library(igraph)
# Clear up data in global environment
rm(list=ls())
# Load data from csv file
fds_copy_pro <- read_csv("C:/Users/sridh/fds copy pro2.csv")
# Check for NAs
sapply(fds_copy_pro, function(x) sum(is.na(x)))
# Overview of original dataset
str(fds_copy_pro)
sapply(fds_copy_pro, typeof)
# randomly sample 1000 rows & remove unnecessary columns
set.seed(830)
fds_copy_pro_sample <-fds_copy_pro[sample(nrow(fds_copy_pro), 1000), -c(1)]
# Format and transform columns
fds_copy_pro_sample$Platform <- as.factor(fds_copy_pro_sample$Platform)
# Double-check format
sapply(fds_copy_pro_sample, typeof)
# * default parameters
processed <- textProcessor(fds_copy_pro_sample$Comments, metadata = fds_copy_pro_sample,
lowercase = TRUE, #*
removestopwords = TRUE, #*
removenumbers = TRUE, #*
removepunctuation = TRUE, #*
stem = TRUE, #*
wordLengths = c(3,Inf), #*
sparselevel = 1, #*
language = "en", #*
verbose = TRUE, #*
onlycharacter = TRUE, # not def
striphtml = FALSE, #*
customstopwords = NULL, #*
v1 = FALSE) #*
# filter out terms that don’t appear in more than 10 documents,
out <- prepDocuments(processed$documents, processed$vocab, processed$meta, lower.thresh=10)
docs <- out$documents
vocab <- out$vocab
meta <-out$meta
# Check levels
levels(meta$Platform)
set.seed(831)
system.time({
First_STM <- stm(docs, vocab, 15,
prevalence =~ Platform,
data = meta,
seed = 15, max.em.its = 5
)
})
# Plot first Topic Model
plot(First_STM)
topic_correlation<-topicCorr(Third_STM)
# Load libraries
library(topicmodels)
library(lda)
library(slam)
library(stm)
library(ggplot2)
library(dplyr)
library(tidytext)
library(future) # Speed up processing
plan(multicore)
library(tm) # Text mining framework
library(tidyverse) # Data manipulation
library(wordcloud) # For word clouds
library(NLP)
library(ldatuning)
library(readr)
library(igraph)
# Clear up data in global environment
rm(list=ls())
# Load data from csv file
fds_copy_pro <- read_csv("C:/Users/sridh/fds copy pro2.csv")
# Check for NAs
sapply(fds_copy_pro, function(x) sum(is.na(x)))
# Overview of original dataset
str(fds_copy_pro)
sapply(fds_copy_pro, typeof)
# randomly sample 1000 rows & remove unnecessary columns
set.seed(830)
fds_copy_pro_sample <-fds_copy_pro[sample(nrow(fds_copy_pro), 1000), -c(1)]
# Format and transform columns
fds_copy_pro_sample$Platform <- as.factor(fds_copy_pro_sample$Platform)
# Double-check format
sapply(fds_copy_pro_sample, typeof)
# * default parameters
processed <- textProcessor(fds_copy_pro_sample$Comments, metadata = fds_copy_pro_sample,
lowercase = TRUE, #*
removestopwords = TRUE, #*
removenumbers = TRUE, #*
removepunctuation = TRUE, #*
stem = TRUE, #*
wordLengths = c(3,Inf), #*
sparselevel = 1, #*
language = "en", #*
verbose = TRUE, #*
onlycharacter = TRUE, # not def
striphtml = FALSE, #*
customstopwords = NULL, #*
v1 = FALSE) #*
# filter out terms that don’t appear in more than 10 documents,
out <- prepDocuments(processed$documents, processed$vocab, processed$meta, lower.thresh=10)
docs <- out$documents
vocab <- out$vocab
meta <-out$meta
# Check levels
levels(meta$Platform)
set.seed(831)
system.time({
First_STM <- stm(docs, vocab, 15,
prevalence =~ Platform,
data = meta,
seed = 15, max.em.its = 5
)
})
# Plot first Topic Model
plot(First_STM)
# Load libraries
library(topicmodels)
library(lda)
library(slam)
library(stm)
library(ggplot2)
library(dplyr)
library(tidytext)
library(future) # Speed up processing
plan(multicore)
library(tm) # Text mining framework
library(tidyverse) # Data manipulation
library(wordcloud) # For word clouds
library(NLP)
library(ldatuning)
library(readr)
library(igraph)
# Clear up data in global environment
rm(list=ls())
# Load data from csv file
fds_copy_pro <- read_csv("C:/Users/sridh/fds copy pro2.csv")
# Check for NAs
sapply(fds_copy_pro, function(x) sum(is.na(x)))
# Overview of original dataset
str(fds_copy_pro)
sapply(fds_copy_pro, typeof)
# randomly sample 1000 rows & remove unnecessary columns
set.seed(830)
fds_copy_pro_sample <-fds_copy_pro[sample(nrow(fds_copy_pro), 1000), -c(1)]
# Format and transform columns
fds_copy_pro_sample$Platform <- as.factor(fds_copy_pro_sample$Platform)
# Double-check format
sapply(fds_copy_pro_sample, typeof)
# * default parameters
processed <- textProcessor(fds_copy_pro_sample$Comments, metadata = fds_copy_pro_sample,
lowercase = TRUE, #*
removestopwords = TRUE, #*
removenumbers = TRUE, #*
removepunctuation = TRUE, #*
stem = TRUE, #*
wordLengths = c(3,Inf), #*
sparselevel = 1, #*
language = "en", #*
verbose = TRUE, #*
onlycharacter = TRUE, # not def
striphtml = FALSE, #*
customstopwords = NULL, #*
v1 = FALSE) #*
# filter out terms that don’t appear in more than 10 documents,
out <- prepDocuments(processed$documents, processed$vocab, processed$meta, lower.thresh=10)
docs <- out$documents
vocab <- out$vocab
meta <-out$meta
# Check levels
levels(meta$Platform)
set.seed(831)
system.time({
First_STM <- stm(docs, vocab, 15,
prevalence =~ Platform,
data = meta,
seed = 15, max.em.its = 5 )
})
# Plot first Topic Model
plot(First_STM)
View(fds_copy_pro)
View(fds_copy_pro_sample)
# Load libraries
library(topicmodels)
library(lda)
library(slam)
library(stm)
library(ggplot2)
library(dplyr)
library(tidytext)
library(future) # Speed up processing
plan(multicore)
library(tm) # Text mining framework
library(tidyverse) # Data manipulation
library(wordcloud) # For word clouds
library(NLP)
library(ldatuning)
library(readr)
library
# Clear up data in global environment
rm(list=ls())
# Load data from csv file
fds_copy_pro <- read_csv("C:/Users/sridh/fds copy pro2.csv")
# Check for NAs
sapply(fds_copy_pro, function(x) sum(is.na(x)))
# Overview of original dataset
str(fds_copy_pro)
sapply(fds_copy_pro, typeof)
# randomly sample 1000 rows & remove unnecessary columns
set.seed(830)
fds_copy_pro_sample <-fds_copy_pro[sample(nrow(fds_copy_pro), 1000), -c(1)]
# Format and transform columns
fds_copy_pro_sample$Platform <- as.factor(fds_copy_pro_sample$Platform)
# Double-check format
sapply(fds_copy_pro_sample, typeof)
# * default parameters
processed <- textProcessor(fds_copy_pro_sample$Comments, metadata = fds_copy_pro_sample,
lowercase = TRUE, #*
removestopwords = TRUE, #*
removenumbers = TRUE, #*
removepunctuation = TRUE, #*
stem = TRUE, #*
wordLengths = c(3,Inf), #*
sparselevel = 1, #*
language = "en", #*
verbose = TRUE, #*
onlycharacter = TRUE, # not def
striphtml = FALSE, #*
customstopwords = NULL, #*
v1 = FALSE) #*
# filter out terms that don’t appear in more than 10 documents,
out <- prepDocuments(processed$documents, processed$vocab, processed$meta, lower.thresh=10)
docs <- out$documents
vocab <- out$vocab
meta <-out$meta
# Check levels
levels(meta$Platform)
set.seed(831)
system.time({
First_STM <- stm(docs, vocab, 15,
prevalence =~ Platform,
data = meta,
seed = 15, max.em.its = 5 )
})
# Plot first Topic Model
plot(First_STM)
set.seed(832)
system.time({
Second_STM <- stm(documents = out$documents, vocab = out$vocab,
K = 18, prevalence =~ Platform ,
max.em.its = 75, data = out$meta,
init.type = "Spectral", verbose = FALSE
)
})
# Plot second Topic Model
plot(Second_STM)
# Find k: Approach 1
set.seed(833)
system.time({
findingk <- searchK(out$documents, out$vocab, K = c(10:30),
prevalence =~ Platform, data = meta, verbose=FALSE
)
})
# Plot
plot(findingk)
# Find k: Approach 2
set.seed(834)
system.time({
findingk_ver2 <- searchK(documents = out$documents,
vocab = out$vocab,
K = c(10,20,30,40,50,60, 70), #specify K to try
N = 500, # matches 10% default
proportion = 0.5, # default
heldout.seed = 1234, # optional
M = 10, # default
cores = 1, # default=1
prevalence =~ Platform,
max.em.its = 75, #was 75
data = meta,
init.type = "Spectral",
verbose=TRUE
)
})
# Plot
plot(findingk_ver2)
# Find k: Approach 3
set.seed(835)
system.time({
findingk_ver3.lee_mimno <- stm(documents = out$documents,
vocab = out$vocab,
K = 0, # K=0 instructs STM to run Lee-Mimno
seed = 1234, # randomness now, seed matters
prevalence =~ Platform,
max.em.its = 75,
data = meta,
init.type = "Spectral",
verbose=TRUE
)
})
# Plot
plot(findingk_ver3.lee_mimno)
# Run final topic model at 20 topics and see how long it takes
set.seed(836)
system.time({
Third_STM <- stm(documents = out$documents, vocab = out$vocab,
K = 20, prevalence =~ Platform,
max.em.its = 75, data = out$meta,
init.type = "Spectral", verbose = FALSE
)
})
#Plot
plot(Third_STM)
# Top Words
labelTopics(Third_STM)
# We can find the top documents associated with a topic with the findThoughts function:
# top 2 paragraps for Topic 1 to 10
findThoughts(Third_STM, texts = meta$Platform,n = 2, topics = 1:10)
# We can look at multiple, or all, topics this way as well.
# For this we’ll just look at the shorttext.
# top 3 paragraps for Topic #1 to 15
findThoughts(Third_STM, texts = meta$Platform,n = 3, topics = 1:15)
# Graphical display of topic correlations
topic_correlation<-topicCorr(Third_STM)
plot(topic_correlation)
# Graphical display of convergence
plot(Third_STM$convergence$bound, type = "l",
ylab = "Approximate Objective",
main = "Convergence")
# Wordcloud:topic 17 with word distribution
set.seed(837)
cloud(Third_STM, topic=17, scale=c(10,2))
# Working with meta-data
set.seed(837)
predict_topics<-estimateEffect(formula = 1:10 ~ Platform + Label,
stmobj = Third_STM,
metadata = out$meta,
uncertainty = "Global",
prior = 1e-5)
# Effect of Zacks vs . Seeking Alpha publishers
set.seed(837)
plot(predict_topics, covariate = "Label", topics = c(1,4,10),
model = Third_STM, method = "difference",
cov.value1 = "Zacks", cov.value2 = "Seeking Alpha",
xlab = "More Seeking Alpha ... More Zacks",
main = "Effect of Zacks vs. Seeking Alpha",
xlim = c(-.1, .1), labeltype = "custom",
custom.labels = c('Topic 1','Topic 4','Topic 10'))
# Effect of 'TalkMarkets' vs. 'Investopedia' publishers
set.seed(837)
plot(predict_topics, covariate = "Label", topics = c(1,4,10),
model = Third_STM, method = "difference",
cov.value1 = "TalkMarkets", cov.value2 = "Investopedia",
xlab = "More Investopedia ... More TalkMarkets",
main = "Effect of TalkMarkets vs. Investopedia",
xlim = c(-.1, .1), labeltype = "custom",
custom.labels = c('Topic 1','Topic 4','Topic 10'))
set.seed(831)
plot(Third_STM,
type="perspectives",
topics=c(17,12),
plabels = c("Topic 17","Topic 12"))
# Topic proportions
plot(Third_STM, type = "hist", topics = sample(1:20, size = 9))
plot(Third_STM, type="hist")
# The topicQuality() function plots these values
# and labels each with its topic number:
topicQuality(model=Third_STM, documents=docs)
View(processed)
View(topic_correlation)
